\chapter{局部路径规划导航方法}
本章将详细介绍LVL-Nav方法中的局部路径规划导航方法，包含描述全局路径规划方法不足之处的引言，针对该不足所提出的局部路径规划方法的导航框架设计，最后分别描述了框架中的特征提取模块、特征融合模块、运动模块和图像点云融合模块四个主体部分。


\section{引言}

目标物体导航方法旨在使移动机器人能根据自然语言指令中的多种目标进行导航，并要求机器人能够导航至目标位置的半米范围内。使机器人能够理解和执行指令是完成这一任务的关键之一，但基于导航点的全局路径规划导航方法侧重于将移动机器人引导到环境中预设的导航点，在许多应用场景中实际选取的导航点可能只是路径上的中转位置，仅仅代表该点距离目标较近而不代表机器人已经到达最终的实际目标旁。出于这类方法的限制，我们所提出的全局路径规划方法在目标没有明确的预设导航点的情况下无法十分有效地应对需要精确导航到目标物体的任务。
除此之外，在未知或动态变化的环境中机器人需要能够根据感知信息实时地调整自己的路径并同时发现和定位到目标物体。传统的导航方法并不具备让机器人在局部环境中进行自主探索的能力，无法完成机器人在未知环境中的探索任务。
因此要在更复杂的应用场景中实现精准的导航，移动机器人不仅需要依赖于导航点的引导，还需要具备自主探索、识别目标并作出相应决策的能力。

在现有的视觉语言导肮方法中，
2020年，Krantz\cite{krantz2020beyond}等人开发了序列到序列的基准模型和能够融合多种特征的多模态注意模型，用以完成连续环境中的视觉语言导航。
Du\cite{du2020learning, du2021vtnet}等人提出了基于视觉神经网络的目标物体导航方法，使用Transformer表征环境特征并输入强化学习网络学习导航策略，降低了模型训练时的试错成本。
Wang\cite{wang2021visual}等人为了解决不同机器人之间难以传递导航技能而提出了一种基于元学习的视觉感知泛化策略，在视觉感知模型中分别使用与模型无关的元学习算法和基于指标的元学习算法，以便在可见和不可见的环境中更好地泛化，使智能体能够快速适应新的相机配置。
Fang\cite{fang2022target}等人提出了一种基于强化学习的目标物体导航方法，提高了智能体的泛化能力。
Fukushima\cite{fukushima2022object}等人提出基于对象记忆Transfomer的目标物体导航方法，使用对象场景存储器存储长期场景和对象序列，实现室内环境高效导航。
朱威\cite{朱威2024结合优势结构和最小目标}等人提出了一种结合优势结构和最小化目标Q值的深度强化学习导航算法，加快多目标连续导航训练过程中的收敛速度。
这类方法基本都采用从视觉观察中提取物体语义、物体位置和相对位置等特征信息，将其通过编码的嵌入层网络以构建丰富的视觉表示，以此来告诉模型周围环境的特征。除此之外模型还会关注当前视觉观察中与目标物体相关联的区域的方向信息，这使得代理能够朝着正确的方向进行探索导航。

根据上述的问题和已有的解决思路，本章提出了一种局部路径规划方法。我们的目标是根据指令中目标物体的名称，通过特征融合、特征提取网络模型输出的离散动作在局部环境中进行自主探索，同时利用视觉图像信息识别出目标物体，在视觉观察中定位目标物体后再由图像点云融合算法计算获得目标的精确位姿，转换坐标系后发布导航任务完成局部环境目标物体导航。该方法由特征提取模块、特征融合模块、运动模块和图像点云融合模块构成。


\section{导航框架设计}
未知环境下视觉导航系统需要实现未知环境探索、动作执行和图像点云融合导航三个方面的功能，因此我们设计了由未知环境探索节点、运动节点、图像点云融合节点构成的局部路径规划导航系统，如图所示\ref{局部路径规划导航方法}。

其中，未知环境探索节点由多模态特征提取器与跨模态融合器一同组成，该节点基于Transformer的并行注意力架构对视觉观察的场景特征向量与目标对象语义描述特征进行融合，进而生成用以描述导航方向指引向量以及表征空间拓扑结构的环境特征。接着，在ROS机器人导航框架之下通过动作执行节点封装可能执行的离散环境探索动作，以期找到导航目标，当发现目标时则进入图像点云融合节点，通过点云聚类算法、目标检测算法和IoU度量算法获得精准的目标物体的位置信息，最后发布导航任务完成目标物体导航。
\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.08]{Fig/未知环境导航框架.png}
    \caption{\label{局部路径规划导航方法}局部路径规划导航框架}
\end{figure}

具体来说，特征提取模块采用预训练参数初始化的残差卷积网络来解析场景的宏观语义表征，同时通过基于注意力机制的检测模型捕获包含局部对象的语义分布及其几何编码信息在内的细粒度区域感知特征，并通过参数化嵌入映射层来生成具有可解释性的目标特征向量。接着，在特征融合模块中对全局特征、局部特征和目标特征进行融合，将获得的方向特征、环境特征和上一时刻的动作特征通过LSTM网络进行编码生成导航动作和动作评分，通过运动模块中封装好的离散动作函数进行执行，以此来指导代理在局部未知环境中进行探索。
接着，在视觉和多线激光雷达进行联合标定并确定发现目标后，通过YOLOV10目标检测算法认知环境中存在的目标的同时，再通过点云聚类感知获得目标与移动机器人之间的相对位置信息，通过图像点云融合模块获得目标位姿，再经过坐标变换得到目标在map坐标系下的坐标。最后再运动模块中利用Navigation导航框架发布导航任务完成导航到目标旁以完成局部导航，进一步完成移动机器人执行导航的闭环任务。

未知环境下的导航探索主要由特征提取模块和特征融合模块构成，如图\ref{提取-融合框架}，其中特征提取模块从移动机器人上的单目相机获取第一人称环境视觉图像，并将其送入不同的特征提取网络中，分别提取表征各个区域物体语义、位置信息的$100 \times 256$局部特征、表征导航环境中机器人所处位置的位姿信息的$49 \times 256$全局特征和表征探索导航目标的$1 \times 256$目标特征。接着再通过主要由encoder特征强化、decoder特征融合和LSTM这三个结构组成的特征融合模块生成表征离散动作空间的动作概率分布函数。具体来说特征融合模块将特征提取获得的三个不同特征进行强化融合，并通过LSTM网络进行导航序列建模以生成$1 \times 4$的动作特征，最后由运动模块进行探索任务。
\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.11]{Fig/提取-融合框架.png}
    \caption{\label{提取-融合框架}未知环境下的导航探索框架}
\end{figure}

上述操作过程可以用公式表示为
\begin{equation}
    {X_{{\rm{local}}}},{X_{{\rm{global}}}},{X_{{\rm{target}}}} = Extract\left( {{X_{{\rm{image}}}},{X_{{\rm{text}}}}} \right)
    \label{myeq30}
\end{equation}
\begin{equation}
    {X_{{\rm{action}}}} = Fusion\left( {{X_{{\rm{local}}}},{X_{{\rm{global}}}},{X_{{\rm{target}}}}} \right)
    \label{myeq31}
\end{equation}
其中$Extract\left( {} \right),Fusion\left( {} \right)$分别表示特征提取模块、特征融合模块，${X_{{\rm{local}}}},{X_{{\rm{global}}}},{X_{{\rm{target}}}},{X_{{\rm{image}}}},{X_{{\rm{text}}}},{X_{{\rm{action}}}}$分别表示特征提取模块中的局部特征、全局特征、目标特征、第一人称视觉图像、输入目标和特征融合模块的动作特征。



\section{特征提取模块}
在未知环境的探索导航过程中使用多层次的特征提取策略能够充分利用环境信息，使机器人在无先验知识的情况下更高效、可靠地探索并接近目标。
在未知环境下的探索导航过程中，特征提取模块主要由三个分别提取局部特征、全局特征和目标特征的不同网络组成，可以用公式表示为
\begin{equation}
{X_{{\rm{local}}}} = Concat\left( {ReLU\left( {Linear\left( {DETR\left( {{X_{{\rm{image}}}}} \right)} \right)} \right),{X_{{\rm{corelation}}}}} \right)
    \label{myeq32}
\end{equation}
\begin{equation}
{X_{{\rm{global}}}} = Flatten\left( {Conv\left( {ResNet\left( {{X_{{\rm{image}}}}} \right)} \right) + {X_{{\rm{position}}}}} \right)
    \label{myeq33}
\end{equation}
\begin{equation}
{X_{{\rm{target}}}} = Embedding\left( {{X_{{\rm{text}}}}} \right)
    \label{myeq34}
\end{equation}

经过特征提取模块处理后，提取出的的用于描述局部场景内物体语义属性及方位信息的区域特征、描述整体环境状态的全局特征以及反映待识别目标语义特性的对象特征均被标准化为256维向量并集成于同一嵌入空间之中。这一对齐操作有效促进了多模态特征的协同交互，为后续特征融合环节构建视觉观测数据与目标实体间的深度语义关联奠定了结构基础，能够实现异构特征的高效融合。

\subsection{提取局部特征}
局部特征的提取流程如图\ref{局部特征提取}所示。在一开始我们使用预训练的DETR网络来提取第一人称视觉图像，用于表征当前视觉观察的局部信息、帮助代理认知视觉观察图像中物体代表的语义信息和位置信息，得到$100 \times 256$的局部特征，其中我们的DETR网络使用ResNet-50作为主干网络，采用六层编码器、六层解码器、八个独立注意力头和维度为2048的前馈神经网络搭建而成。然后将该特征经过全连接层Linear和ReLU激活函数以得到$100 \times 249$的局部特征，再将表征推理结果的$100 \times 7$关联特征与局部特征进行连接，得到包含环境所存在的物体语义信息、位姿信息、可靠性在内的$100 \times 256$的局部特征。
\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.09]{Fig/局部特征提取.png}
    \caption{\label{局部特征提取}局部特征提取流程}
\end{figure}

\subsection{提取全局特征}


全局特征的提取流程如图\ref{全局特征提取}所示。我们构建了基于ResNet18架构的特征提取网络用于解析机器人本体视角采集的RGB图像数据。该编码器通过深度卷积网络提取场景的空间拓扑表征来为智能体提供空间定位的环境认知信息。首先使用斯坦福大学开源的ImageNet数据集对我们搭建的ResNet18网络进行预训练，将第一人称观察视觉图像输入到该预训练的网络中再经过Conv卷积层和ReLU激活函数得到$7 \times 7 \times 256$的全局特征，然后将输出的环境表征张量与三角函数位置嵌入生成的$7 \times 7 \times 256$维位置编码矩阵进行Add操作和Flatten操作得到维度为$49 \times 256$的融合特征向量以完成多模态感知信息的空间对齐。其中正余弦位置编码是一种固定、无训练参数的编码方法，它可以表示为式\ref{myeq35}、\ref{myeq36}，其中pos表示位置索引，i表示特征维度索引，$d_{{\rm{model}}}$表示嵌入维度即模型隐藏层维度，分母${10000^{{{2i} \mathord{\left/
 {\vphantom {{2i} {{d_{{\rm{model}}}}}}} \right.
 \kern-\nulldelimiterspace} {{d_{{\rm{model}}}}}}}}$表示用于确保不同维度的编码值在不同的频率范围内变化的缩放因子。
\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.09]{Fig/全局特征提取.png}
    \caption{\label{全局特征提取}全局特征提取流程}
\end{figure}

\begin{equation}
    P{E_{\left( {pos,2i} \right)}} = \sin \left( {\frac{{pos}}{{{{10000}^{{{2i} \mathord{\left/
 {\vphantom {{2i} {{d_{{\rm{model}}}}}}} \right.
 \kern-\nulldelimiterspace} {{d_{{\rm{model}}}}}}}}}}} \right)
    \label{myeq35}
\end{equation}
\begin{equation}
    P{E_{\left( {pos,2i + 1} \right)}} = \cos \left( {\frac{{pos}}{{{{10000}^{{{2i} \mathord{\left/
 {\vphantom {{2i} {{d_{{\rm{model}}}}}}} \right.
 \kern-\nulldelimiterspace} {{d_{{\rm{model}}}}}}}}}}} \right)
    \label{myeq36}
\end{equation}

\subsection{提取目标特征}
目标特征提取流程如图\ref{目标特征提取}所示。我们在特征构建阶段定义词表容量$V = 32$与嵌入空间维度D=256作为核心超参数并以此初始化嵌入矩阵$M$，接着通过输入目标实体在词表中的离散索引$I$来执行嵌入层的索引查询操作，从映射关系矩阵$M$中抽取对应的目标语义向量进而完成符号空间到连续向量空间的转换。在训练过程中，该目标特征会不断被优化，使语义相近的信息在嵌入空间中更加接近，从而增强特征融合模块在构建文本语义与视觉语义关联方面的能力。利用嵌入层网络对目标物体的单词进行编码可以生成用于指代目标物体语义的目标特征。在后续的特征融合过程中，可以借助注意力机制筛选出视觉观察中与目标物体相关的物体特征，从而精准确定导航的方向以快速在未知环境中找到目标。
\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.10]{Fig/目标特征提取.png}
    \caption{\label{目标特征提取}目标特征提取流程}
\end{figure}



\section{特征融合模块}
特征融合模块主要由特征融合编码器、特征融合解码器和LSTM网络构成。其中特征融合编码器主要由具有自注意力机制的Transformer encoder组成，它将特征提取模块获得的局部特征通过自注意力机制编码强化，获得能够表征表示物体与一和位置信息的强化局部特征，并将其作为键值对输入到特征融合解码器之中。特征融合解码器同样由具有多头自注意力机制的Transfomer decoder组成，它将表征目标物体语义信息和机器人所处环境位置信息的目标特征和全局特征进行强化编码，然后将作为键值对的局部特征与他们连结而成查询一同输入到网络之中，通过decoder进行融合得到环境特征和方向特征，最后，经过LSTM网络输出代理在未知环境下进行探索的离散导航动作。

我们通过观察人类在环境中寻找目标物体的过程发现，当目标物体出现在视觉观察范围内时，我们会直接按照最近的路线去靠近目标，而当目标物体未出现在观察范围内时，我们则会前往环境中存在的所有物体中与目标关联性最强的物体附近进行寻找。我们按照这样的思路去搭建特征融合网络以指导代理能够更精准的在局部未知环境中进行导航，如图\ref{未知环境导航示例图}所示，环境中存在硬纸盒、手提电脑和花盆三个物体。当需要导航到视觉观察中所发现的目标物体花盆时，代理就会根据标目标物体出现在视觉观察中的位置进行决策，执行向右前方进行移动的动作。当需要导航到视觉观察中未出现的目标物体鼠标时，代理则会先判断视觉观察中所出现的所有物体与目标物体的关联性，然后导航到手提电脑这一最可能会找到鼠标的目标旁。
\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.10]{Fig/导航示例图.png}
    \caption{\label{未知环境导航示例图}未知环境导航示例图}
\end{figure}

因此，为了使移动机器人能够正确定位其当前所在位置，需要从视觉观察中提取环境特征，同时还需要提取局部特征以感知周围环境中物体的语义信息和位置信息。此外，代理应建立目标特征与全局特征、局部特征之间的紧密关联，从而有效引导方向特征的生成。同时，还需融合当前的全局特征与局部特征以获取环境的整体表征，并构建方向特征与环境特征相对应的序列，为后续模块提供历史经验信息，帮助LSTM网络正确地输出导航动作以指导代理在局部未知环境中进行精准高效的导航。特征融合模块的操作流程可以用如下公式表示，其中$Encoder$表示特征融合编码器，$Decoder$表示特征融合解码器。
\begin{equation}
{X_{{\rm{encoder}}}} = Encoder\left( {{X_{{\rm{local}}}}} \right)
    \label{myeq37}
\end{equation}
\begin{equation}
    {X_{{\rm{direction}}}},{X_{{\rm{environment}}}} = Decoder\left( {Concat\left( {{X_{{\rm{global}}}},{X_{{\rm{target}}}},{X_{{\rm{parameter}}}}} \right),{X_{{\rm{encoder}}}}} \right)
    \label{myeq38}
\end{equation}
\begin{equation}
    X_{{\rm{action}}}^n = Linear\left( {LSTM\left( {{X_{{\rm{direction}}}},{X_{{\rm{environment}}}},X_{{\rm{action}}}^{n - 1}} \right)} \right)
    \label{myeq39}
\end{equation}

\subsection{特征融合编码器}
特征融合模块中的编码、解码器基于Transfomer进行搭建，它的参数如表\ref{特征融合tf参数}所示，即由输入维度为256、层数为2、自注意力头为8、前馈神经网络模型的维度为512共同组成。encoder结构如图\ref{encoder}所示，我们将$100 \times 256$维的局部特征作为输入进入到encoder中的自注意力层中，经过第一次的残差连接、层归一化和前馈神经网络后经过第二次的残差连接和曾归一化，输出得到强化后的$100 \times 256$维局部特征。在encoder强化表征环境中局部物体的语义信息和未知信息的局部特征的过程之中，分为100个维度的256个序列的每一个序列都通过网络的自注意力机制与其他序列进行交互，以学习环境中物体之间的关联信息，帮助在当前视觉观察环境中未能发现目标的代理朝着与目标物体最相关联的物体方向前进，进而发现目标物体。
\begin{table}
\caption{\label{特征融合tf参数}特征融合Transfomer网络主要参数}
\centering
\small
\begin{tabular}{cccc}
    \hline
    n\_head & num\_encoder\_layers & num\_decoder\_layers & dim\_feedforward \tabularnewline 
    \hline 
    8 & 2 & 2 & 512 \tabularnewline
    \hline 
\end{tabular}
\end{table}

\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.10]{Fig/encoder.png}
    \caption{\label{encoder}encoder结构图}
\end{figure}


\subsection{特征融合解码器}
特征融合的解码器decoder结构如图\ref{decoder}所示。我们将特征提取模块输出表征代理所处环境的$49 \times 256$维全局特征、表征当前导航目标的$1 \times 256$维目标特征和可学习的$1 \times 256$维目标特征进行连结，得到$51 \times 256$维的特征输入到decoder之中，在经过第一个多头自注意力层、残差连接、层归一化之后，将Transfomer encoder输出的$100 \times 256$维强化后的局部特征作为键值对、$51 \times 256$维的特征作为查询一同输入到第二个多头自注意力层之中，经过残差连接归一化层、前馈神经网络和最后一次的残差连接和层归一化后，得到$51 \times 256$维的特征，使用该特征中与$1 \times 256$维的目标特征和$1 \times 256$维的可学习的参数特征相对应位置的向量，作为表征环境中与目标相关联物体的信息，以帮助代理确定导航方向的方向特征，还有表征全局特征和局部特征共同联合的环境特征。
\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.08]{Fig/decoder.png}
    \caption{\label{decoder}decoder结构图}
\end{figure}


\subsection{LSTM网络}
LSTM是一种能够有效地捕捉序列数据中的长期依赖关系的特殊循环神经网络，传统的神经网络或其他类型的RNN可能会遗忘过去的特征信息，而LSTM通过引入遗忘门、输入门和输出门从而有效地解决了这一问题，这种循环结构使得它在基于历史信息进行决策或是当任务需要依赖较长时间之前的状态时的场景中表现出色。而在机器人执行导航这种需要处理时序信息和长期依赖的任务的过程中，需要代理根据上一时刻的导航动作、方向特征和环境特征提取有用的历史信息，使用LSTM结构的网络根据以往的经验做出更精准的决策，从而达到显著提高导航系统的决策能力的目的。

通过LSTM网络输出局部未知环境下的导航动作流程如图\ref{LSTM}所示。我们将特征融合解码器输出的$1 \times 256$维方向特征、$1 \times 256$维环境特征和表示未知环境下代理所执行的$1 \times 4$维导航动作进行拼接，得到$1 \times 516$维特征向量作为双层LSTM网络的输入，最终输出$1 \times 4$维的导航动作交由运动模块所封装的导航动作执行，并且该当前时刻的导航动作在经过一个线性层后作为下一时刻的动作特征输入到LSTM网络中。
\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.10]{Fig/LSTM.png}
    \caption{\label{LSTM}LSTM结构图}
\end{figure}


\section{运动模块}
在运动模块中，我们通过ROS的分布式通信框架来订阅目标位姿信息，依托差分驱动模型来实现机器人在环境中的移动探索。


本系统研发的移动机器人基于差速转向原理实现底盘驱动。在机械结构设计上，我们通过独立控制两侧驱动电机的转速差异来完成运动控制。当两侧驱动轮保持相同转速时机器人将保持直线行驶；当两侧转速产生差值时，根据差速传动特性机器人会以自身轴线为中心进行转向运动。基于这种原理，我们可以通过调整双轮速度参数实现不同曲率的弧线运动与精确转向定位。在制动控制方面，驱动系统可通过切断电机供电使设备实现急停功能。

具体如图\ref{motion_control}所示，底盘控制系统的工作流程包含指令下发与动态修正两个关键环节。主控制器首先将预设转速参数传输至电机驱动模块，但由于机械传动间隙、地面摩擦系数变化等客观因素，机器人车轮的实际转速值与理论设定值往往存在偏差。因此系统设计了一种闭环控制架构，驱动模块不仅需要输出PWM控制信号，还通过正交编码器实时采集电机转动参数。在10ms的采样周期内，控制模块会持续统计编码器脉冲数量并将其转换为移动距离参数进行反馈。主控单元运用运动学方程对这些原始数据进行坐标变换与轨迹拟合，最终生成毫米级精度的实时位姿信息从而保障移动平台的精准定位能力。


\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.07]{Fig/机器人底盘运动控制.png}
    \caption{\label{motion_control}机器人的运动控制方法}
\end{figure}

在控制移动机器人移动的过程中通常会受到地面摩擦阻力的影响，出现左右轮的实际转速和想要控制的速度不同的情况，导致移动机器人无法按照计划的轨迹进行移动，这时候就需要加入PID控制算法来调整小车的运动，使其能够精准的按照目标轨迹进行移动。

PID控制器是一种广泛使用的反馈控制系统，能够根据目标值与当前值之间的偏差来调整控制变量，从而使系统稳定地达到设定目标。具体而言，PID控制器通过调节比例、积分和微分来控制机器人的运动，确保其到达预定的位置或保持稳定的姿态，如图\ref{pid}，它简单易用，适用于大多数线性控制问题，具有很好的实时性和鲁棒性。
\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.10]{Fig/pid.png}
    \caption{\label{pid}PID算法控制电机转速}
\end{figure}

比例项是PID控制器中最基础的组成部分，它直接根据期望值与实际值之间的偏差来进行调整，如式\ref{myeq17}，其中$P$表示比例控制输出，${K_p}$表示比例增益，即在误差变化时控制器输出的变化幅度，$e\left( t \right)$表示当前时刻的误差，当误差越大时，比例项的输出也就越大，以帮助系统更快速的接近目标值。
\begin{equation}
    P = {K_p} \cdot e\left( t \right)
    \label{myeq17}
\end{equation}

积分项是PID控制器中的第二个组成部分。积分项的目的是消除系统在长时间运行中可能出现的静态误差，确保系统达到并保持在期望的目标位置，如式\ref{myeq18}，其中$I$表示积分控制输出，${K_i}$表示积分增益，能够决定积分项对控制输出的影响程度，${e\left( t \right)}$表示误差。当系统的输出长期无法达到设定的目标时，积分项就会逐渐地累积来弥补系统长期积累的偏差，一直到该误差消除。
\begin{equation}
    I = {K_i} \cdot \int_0^t {e\left( t \right)dt} 
    \label{myeq18}
\end{equation}

当积分增益过大时会导致系统响应过度，引发系统的超调和震荡，这时候就需要通过微分项来预测误差的变化趋势以减少系统发生超调和震荡的可能。与前面介绍的两者不同，微分项主要关注的是误差变化的速度而非当前的误差值，如式\ref{myeq19}，其中$D$表示微分控制输出，${K_d}$表示决定了微分项对控制输出的影响程度的微分增益，$\frac{d}{{dt}}e\left( t \right)$表示用来预测未来误差变化趋势的误差变化速率。通过微分项的预测误差和提前调整，系统能够避免震荡和过调现象的发生，让系统更加平稳的响应。
\begin{equation}
    D = {K_d} \cdot \frac{d}{{dt}}e\left( t \right)
    \label{myeq19}
\end{equation}
通常情况下积分项、比例项和微分项会一起使用，即通过比例项消除实时误差，利用积分项来消除长期累计的误差，而微分项则通过对误差变化率的敏感度调整来预测误差的变化趋势，减少系统发生超调和震荡的概率，三者相辅相成一同构成一个完整的PID控制器，使其能够更精准、更平稳地调节系统响应。

在移动机器人根据多特征融合网络的输出进行局部范围内的探索以期找到目标物体时，我们需要计算出每一时刻的里程计数据，并将其与订阅获取的实时里程计数据进行比较，从而计算出当前已经移动的距离，达到让机器人准确地进行移动的目的。具体来说里程计包括位姿(位置和方向角)和运动速度(线速度与角速度)两个重要的信息，它的计算过程如图\ref{motion_calculate}所示。在整个机器人运动的过程当中，假设用${p_i}$来表示在时刻$i$的位姿，图中的${p_1},{p_2}, \ldots ,{p_n}$则代表机器人的整个运动过程中的轨迹。在很短的时间内，机器人从${p_1}$运动到${p_2}$，那么我们就可以根据机器人当前的位姿${p_1}$及其左右轮的速度${V_{left}}$和${V_{right}}$，通过微积分的方法来推算机器人下一个时间步${p_2}$处的线速度、角速度及其位姿，从而计算获得完整的里程计数据。
\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.05]{Fig/移动控制.png}
    \caption{\label{motion_calculate}里程计计算}
\end{figure}

在机器人移动的三维空间中存在多个坐标系，我们构建了存在于导航环境中相对静止的全局坐标系作为机器人本体运动的参考坐标系，在任意的时刻$t$，机器人的位姿可以表示为${P_t} = \left( {{x_t},{y_t},{\theta _t}} \right)$，其中$\left( {{x_t},{y_t}} \right)$表示机器人的位姿在世界坐标系x、y平面上的投影坐标，${{\theta _t}}$表示朝向角。导航系统则基于差分轮式运动学模型进行驱动，该模型将平台的运动学状态解耦为绕瞬态旋转中心(Instantaneous Center of Rotation, ICR)的刚体转动与平移分量的合成运动，其运动可以近似视为围绕某一瞬时旋转中心，以半径r沿圆弧轨迹运动。已知左右轮的速度${V_{left}}$、${V_{right}}$及两轮间距$l$，可以通过式\eqref{myeq20}、\eqref{myeq21}和\eqref{myeq22}来计算机器人在$t$时刻的线速度$v$、角速度$\omega $以及旋转半径$r$
\begin{equation}
    v = \frac{{{v_{left}} + {v_{right}}}}{2}
    \label{myeq20}
\end{equation}
\begin{equation}
    \omega  = \frac{{{v_{right}} - {v_{left}}}}{l}\left( {{{rad} \mathord{\left/
 {\vphantom {{rad} s}} \right.
 \kern-\nulldelimiterspace} s}} \right)
    \label{myeq21}
\end{equation}
\begin{equation}
    r = \frac{v}{\omega } = \frac{l}{2} \cdot \frac{{{v_{right}} + {v_{left}}}}{{{v_{right}} - {v_{left}}}}
    \label{myeq22}
\end{equation}


在实际计算中为了推算出机器人在当前时刻的位姿，我们基于速度积分法进行计算。设在t时刻移动机器人的位姿为${P_t} = \left( {{x_{t}},{y_{t}},{\theta _{t}}} \right)$，在微笑的时间序间隔$\Delta t$内进行的运动过程可以等效为恒定速度的匀速运动，因此可以通过如下递推式\eqref{myeq23}、\eqref{myeq24}和\eqref{myeq25}计算出结果。
\begin{equation}
{\theta _{t + 1}} = {\theta _{t}} + \omega  * \Delta t
    \label{myeq23}
\end{equation}
\begin{equation}
    {x_{t + 1}} = {x_{t}} + r * \left( {\sin {\theta _{t + 1}} - \sin {\theta _{t}}} \right)
    \label{myeq24}
\end{equation}
\begin{equation}
{y_{t + 1}} = {y_{t}} - r * \left( {\cos {\theta _{t + 1}} - \cos {\theta _{t}}} \right)
    \label{myeq25}
\end{equation}
该方法能够高效、准确地计算机器人每个时刻的位姿，并依次累积得到完整的运动轨迹，为导航和路径规划提供精确的里程计数据支持。在具体的实现中我们使用面向对象的编程思想进行封装，将其用于探索未知环境。

\section{图像点云融合模块}
未知环境的目标物体导航任务要求代理要具备识别出环境中存在着的目标物体的功能。多线激光雷达具备高精度的深度测量能力，它能够提供目标的距离感知信息和三维结构，但点云数据会随着距离的变大而逐渐变得稀疏，在追踪远距离目标时效果有限。单目相机则具有高分辨率的视觉信息，能够捕捉目标的纹理、颜色和边缘特征等认知信息，但它缺乏直接的深度感知能力，同时也容易受到光照条件等环境因素的影响。相比于使用单一传感器进行目标检测，使用多线激光和单目相机的联合检测方法在各种应用场景中展现出显著的优势，能够有效降低误检和漏检率。首先图像点云融合方法能够显著提升代理对远距离目标的检测能力，即使在单目相机难以判别目标的远距离区域依旧可以通过多线激光雷达来获取准确的深度信息。此外目前的视觉言语导航方法大多仅依赖于网络模型输出的动作进行决策，使用导航至目标3米内即算完成导航任务这一笼统指标并不利于后续利用机械臂执行的下游任务的执行，这就需要多线激光提供的准确距离信息来辅助代理进行导航以完成导航至目标半米内的任务。

局部路径规划方法中的图像点云融合模块使用了第二章所介绍的YOLOV10目标检测网络、优化后的欧式点云聚类算法和多线激光单目相机联合标定方法。多线激光获取的点云在经过点云聚类之后通过外参矩阵重投影到目标检测二维图像上，并且使用IoU重叠度(Intersection over Union)来判断目标检测框中的目标物体与点云聚类结果是否匹配，当点云聚类结果与目标检测框不匹配时就会被舍弃，当结果匹配时就会被纳入结果集，并将出所选定的区域框中所有点云的均值作为该目标的距离，用以发布最终导航目标点完成完整的导航。

IoU具有非负性、不可分辨的同一性、对称性和尺度不变这类属性，这使得通过这个方法来判断两个任意形状A和B之间的相似性与它们的空间尺度无关，所以它被广泛用作计算机视觉中许多任务的评估指标，如像素级图像分割、2D和3D对象检测等。
因此，本文使用IoU度量法来衡量有限的点云聚类算法结果和目标检测算法结果之间的相似性。具体来说针对两个有限样本集合$A$和$B$，他们之间的IoU被定义为他们的交集除以他们之间的并集，如式\ref{myeq26}。
\begin{equation}
    {\rm{IoU}}\left( {A,B} \right) = \frac{{A \cap B}}{{A \cup B}} = \frac{{A \cap B}}{{\left| A \right| + \left| B \right| - A \cap B}}
    \label{myeq26}
\end{equation}

根据点云聚类和目标检测的结果可以获得两个不同的预选框，将它们通过预选框匹配算法计算他们之间的IoU度量，如算法\ref{algorithm3}。
当预选框之间的IoU低于0.5时，根据非极大值抑制准则将两者判定为独立目标；当IoU处于区间$0.4<=IoU<=0.8$时，根据置信度加权策略选取两个候选框的重叠区域作为最终检测框；对于IoU大于0.8的强相关预选框则通过最小外接矩形算法生成融合后的最终检测框，如图\ref{iou}。
\begin{algorithm}[!h]
    \caption{IoU度量算法}
    \label{algorithm3}
    \renewcommand{\algorithmicrequire}{\textbf{Input:}}
    \renewcommand{\algorithmicensure}{\textbf{Output:}}
    \renewcommand{\algorithmiccomment}[1]{\hfill $\triangleright$ #1}
    \begin{algorithmic}[1]
        \REQUIRE 检测框的四个角坐标：
        $\begin{array}{l}
            {A_1}\left( {{x_1},{y_1}} \right),{B_1}\left( {{x_2},{y_1}} \right),{C_1}\left( {{x_2},{y_2}} \right),{D_1}\left( {{x_1},{y_2}} \right)\\
            {A_1}\left( {{{x'}_1},{{y'}_1}} \right),{B_1}\left( {{{x'}_2},{{y'}_1}} \right),{C_1}\left( {{{x'}_2},{{y'}_2}} \right),{D_1}\left( {{{x'}_1},{{y'}_2}} \right)\\
            {x_1} \le {x_2},{y_2} \le {y_1},{{x'}_1} \le {{x'}_2},{{y'}_2} \le {{y'}_1}
            \end{array}$  %%input
        \ENSURE 两个预选框之间的IoU结果   %%output
        \STATE  The area of ${{\rm{B}}_{{\rm{c }}}}$:${\rm{Are}}{{\rm{a}}_{\rm{c}}} = ({x_2} - {x_1}) \times ({y_1} - {y_2})$
        \STATE  The area of ${{\rm{B}}_{{\rm{d }}}}$:${\rm{Are}}{{\rm{a}}_{\rm{d}}} = ({{x'}_2} - {{x'}_1}) \times ({{y'}_1} - {{y'}_2})$
		\STATE  The area of overlap:\\
        ${\rm{Are}}{{\rm{a}}_{{\rm{overlap}}}} = (\max ({x_2} - {{x'}_2}) - \min ({x_1},{{x'}_1})) \times (\max ({y_1} - {{y'}_1}) - \min ({y_2},{{y'}_2}))$
		\STATE  ${\rm{IoU}} = {\rm{Are}}{{\rm{a}}_{{\rm{overlap}}}}/({\rm{Are}}{{\rm{a}}_c} + {\rm{Are}}{{\rm{a}}_d} - {\rm{Are}}{{\rm{a}}_{{\rm{overlap}}}})$
    \end{algorithmic}
\end{algorithm}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.85\textwidth]{Fig/iou.png}
    \caption{\label{iou}预选框选择策略}
\end{figure}

在经过预选框选择策略之后获得的最终边界框将用于测定目标在机器人坐标系下的位姿，如图\ref{target_pose}。通过最终边界框的中点$\left( {{u_{mid}},{v_{mid}}} \right)$和单目相机的水平视域、垂直视域，可以分别计算出目标物体与机器人$x$轴正方向的水平夹角$\phi $、目标物体与机器人$y$轴正方向的水平夹角$\varphi $。然后将投影在最终边界框中的点云距离进行均值相加以获得机器人与目标之间的距离$D$，根据$\phi $、$\varphi $和$D$可以计算出机器人坐标系下目标的位姿。
\begin{figure}[htbp]
    \centering
    \includegraphics[scale=0.12]{Fig/目标坐标..png}
    \caption{\label{target_pose}根据平面成像与点云数据求目标位姿}
\end{figure}
最后，通过ROS框架中tf话题所提供的数据和四元组转欧拉角方法即可将机器人坐标系下的目标位姿转换成地图坐标系下的目标位姿，发布导航即可完成完整的目标物体导航任务。需要注意的是，在实验的过程中我们发现当目标物处于桌子上等不可达的位置时，移动机器人将根据AMCL规划出的路径移动到环境的边界处，这通常与环境中离目标最近的导航点不一致，因此我们在发布最后的导航点时会先检查该点是否可达，当其不可达时图像点云融合模块将输出环境中与最终目标导航点相距最近的可达点作为替代。
具体的仿真环境、现实环境和消融实验实验结果见第五章。





\section{本章小结}
本章提出了一种局部路径规划方法，该方法由特征提取模块、特征融合模块、运动模块和图像点云融合模块构成。首先根据指令中目标物体的名称，通过特征融合、特征提取网络模型输出的离散动作，依靠运动节点在局部环境中进行自主探索，同时利用视觉图像信息识别出目标物体，在视觉观察中定位目标物体后再由图像点云融合算法计算获得目标的精确位姿，转换坐标系后发布导航任务完成局部环境目标物体导航。
